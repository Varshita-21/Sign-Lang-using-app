# -*- coding: utf-8 -*-
"""gesture_speech.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jfg_jzRwZOgBa_UgKJSBZ4JgFL8mv47Q
"""

# Commented out IPython magic to ensure Python compatibility.
import tensorflow as tf
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout
from keras.optimizers import Adam, SGD, RMSprop, Adamax
from keras.metrics import categorical_crossentropy
from keras.preprocessing.image import ImageDataGenerator
import pandas as pd
from tensorflow.keras.models import Model
import warnings
import numpy as np
import cv2
import imageio
import numpy as np
from skimage.io import imread, imshow
from skimage.filters import prewitt_h,prewitt_v
import matplotlib.pyplot as plt
# %matplotlib inline
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import ModelCheckpoint, EarlyStopping
warnings.simplefilter(action='ignore', category=FutureWarning)
import glob,os
import sklearn
import sys
import matplotlib.pyplot as plt

import zipfile
from google.colab import drive
drive.mount('/content/gdrive')
root_path = 'gdrive/My Drive/GENERATED_DATASET/'
#test_data_dir=root_path+'Testing'

from skimage.color import rgb2gray
def create_dataset(img_folder):
   
    img_data_array=[]
    class_name=[]
    df=pd.DataFrame(columns=['Image','Label'])
    i=0   
    for dir1 in os.listdir(img_folder):
        for file in os.listdir(os.path.join(img_folder, dir1)):
       
            image_path= os.path.join(img_folder, dir1,  file)
            image= cv2.imread(image_path)
            
            image=rgb2gray(image)
            image=cv2.resize(image, (300,300))
            
            image=np.array(image,dtype="object")

            image = image.astype('float32')
            image /= 255 
            #plt.imshow(image)
            #plt.show()
            #print(image)
            df.loc[i,'Image']=image
            df.loc[i,'Label']=dir1
            #img_data_array.append(image)
            #class_name.append(dir1)
            i+=1
    #return np.array(img_data_array), np.array(class_name)
    return df
X,y =create_dataset(data_dir)

"""**Exploratory** **Image** **Data** **Analytics**"""

pic = imageio.imread('A70.jpg')
plt.figure(figsize = (6,6))
plt.imshow(pic)

print('Type of the image : ' , type(pic)) 
print('Shape of the image : {}'.format(pic.shape)) 
print('Image Hight {}'.format(pic.shape[0])) 
print('Image Width {}'.format(pic.shape[1])) 
print('Dimension of Image {}'.format(pic.ndim))

print('Image size {}'.format(pic.size)) 
print('Maximum RGB value in this image {}'.format(pic.max()))
print('Minimum RGB value in this image {}'.format(pic.min()))

plt.title('R channel') 
plt.ylabel('Height {}'.format(pic.shape[0])) 
plt.xlabel('Width {}'.format(pic.shape[1])) 
plt.imshow(pic[ : , : , 0])
plt.show()

plt.title('G channel')
plt.ylabel('Height {}'.format(pic.shape[0])) 
plt.xlabel('Width {}'.format(pic.shape[1])) 
plt.imshow(pic[ : , : , 1]) 
plt.show()

plt.title('B channel') 
plt.ylabel('Height {}'.format(pic.shape[0])) 
plt.xlabel('Width {}'.format(pic.shape[1])) 
plt.imshow(pic[ : , : , 2]) 
plt.show()

#color value histograms for pixel matrix from image
import seaborn as sns
sns.distplot(pic[:,:,0], bins=12)
sns.distplot(pic[:,:,1], bins=12)
sns.distplot(pic[:,:,2], bins=12)

#Splitting the image into separate color components
fig, ax = plt.subplots(nrows = 1, ncols=3, figsize=(15,5))  
for c, ax in zip(range(3), ax):           
     split_img = np.zeros(pic.shape, dtype="uint8")      
     split_img[ :, :, c] = pic[ :, :, c]      
     ax.imshow(split_img)

pic = imageio.imread('A70.jpg')
def find_mean_img(full_mat, title, size = (40,48)):
    # calculate the average
    mean_img = np.mean(full_mat, axis = 0)
    # reshape it back to a matrix
    mean_img = mean_img.reshape(size)
    plt.imshow(mean_img, vmin=0, vmax=255, cmap='Greys_r')
    plt.title(f'Average {title}')
    plt.axis('off')
    plt.show()
    return mean_img

norm_mean = find_mean_img(pic, 'IMAGE')

from sklearn.decomposition import PCA
from math import ceil
p=pic[ : , : , 1]
def eigenimages(full_mat, title, n_comp = 0.7, size = (16,16)):
    # fit PCA to describe n_comp * variability in the class
    pca = PCA(n_components = n_comp, whiten = True)
    pca.fit(full_mat)
    print('Number of PC: ', pca.n_components_)
    return pca
  
def plot_pca(pca, size = (20,32)):
    # plot eigenimages in a grid
    n = pca.n_components_
    fig = plt.figure(figsize=(8, 8))
    r = int(n**.5)
    c = ceil(n/ r)
    for i in range(n):
        ax = fig.add_subplot(r, c, i + 1, xticks = [], yticks = [])
        ax.imshow(pca.components_[i].reshape(size), cmap='Greys_r')
    plt.axis('off')
    plt.show()   
plot_pca(eigenimages(p, 'Image'))

#reading the image as gray
pic= imread('A70.jpg',as_gray=True)
#calculating horizontal edges using prewitt kernel
edges_prewitt_horizontal = prewitt_h(pic)
#calculating vertical edges using prewitt kernel
edges_prewitt_vertical = prewitt_v(pic)
imshow(edges_prewitt_vertical, cmap='gray')
plt.title('Edge Image')

#reading the image as gray
pic= imread('C66.jpg',as_gray=True)
#calculating horizontal edges using prewitt kernel
edges_prewitt_horizontal = prewitt_h(pic)
#calculating vertical edges using prewitt kernel
edges_prewitt_vertical = prewitt_v(pic)
imshow(edges_prewitt_vertical, cmap='gray')
plt.title('Edge Image')

#l1=os.listdir(train_data_dir)
#for i in l1:
p1=os.path.join(data_dir,'A','A2649.jpg')
p2=os.path.join(data_dir,'B','B2648.jpg')
img=cv2.imread(p1)
#img1=cv2.imread(p2)
#img=data_dir+'/A/A1.jpg'
#cv.RunningAvg(image, acc, alpha)
#plt.imshow(img)
#plt.show()
#plt.imshow(img1)
#plt.show()
averageValue1 = np.float32(img) 
img1=cv2.accumulateWeighted(img, averageValue1, 0.02) 
      
    # converting the matrix elements to absolute values  
    # and converting the result to 8-bit.  
resultingFrames1 = cv2.convertScaleAbs(img1) 
  
    # Show two output windows 
    # the input / original frames window 
plt.imshow(img) 
plt.show()
    # the window showing output of alpha value 0.02 
plt.imshow(resultingFrames1) 
plt.show()

from datetime import datetime
img_width,img_height =224,224
input_shape=(img_width,img_height,3)
batch_size=32

#help(ImageDataGenerator)
#Batch size should not be 1
train_datagen=ImageDataGenerator(rescale=1./255,zoom_range=[0.5,1.0],brightness_range=[0.2,1.0],rotation_range=45,width_shift_range=0.1,height_shift_range=0.2,validation_split=0.2)
test_datagen=ImageDataGenerator(rescale=1./255, validation_split=0.2)

#train_datagen.fit(X_train)
#test_datagen.fit(X_test)
train_generator = train_datagen.flow_from_directory(
    root_path,
    target_size=(img_width, img_height),
    subset="training",
    shuffle=True,
    batch_size=batch_size)
#train_datagen.fit(train_generator)

test_generator = test_datagen.flow_from_directory(
    root_path,
    target_size=(img_width, img_height),
    subset="validation",
    shuffle=False,
    batch_size=batch_size)

class_names = train_generator.classes
print(class_names)
x,y = train_generator.next()
print(train_generator.class_indices)
print(x.shape,y.shape)
#print(x,y)
for i in range(0,1):
    image = x[i]
    #print(x[i])
    plt.imshow((image))
    plt.show()
    print(y[i])
#np.array(lambda x: tf.image.rgb_to_grayscale(x))
def schedule(epoch, lr):
  if epoch < 20:
    return lr
  else:
    return lr * tf.math.exp(-0.1)
#logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
logdir = "logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)
lrscheduler_callback=tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)
reduceplat_callback=tf.keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.1, patience=2, verbose=0,mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir logs

#MobileNetv2 = SGD - 60% - 20 epochs
baseModel = tf.keras.applications.MobileNetV2(weights="imagenet", include_top=False,input_shape=input_shape)
baseModel.trainable=False
headModel = baseModel.output
#headModel = AveragePooling2D(pool_size=(7, 7))(headModel)
headModel = Flatten(name="flatten")(headModel)
headModel=BatchNormalization()(headModel)
headModel = Dense(512, activation="relu")(headModel)
headModel = Dropout(0.5)(headModel)
headModel = Dense(128, activation="relu",kernel_regularizer=tf.keras.regularizers.l2(0.003))(headModel)
headModel = Dropout(0.5)(headModel)
#headModel=BatchNormalization()(headModel)
headModel = Dense(28, activation="softmax")(headModel)
model = Model(inputs=baseModel.input, outputs=headModel) 
#print(model.summary())
model.compile(optimizer =Adamax(lr=0.0001), loss = 'categorical_crossentropy', metrics = ['acc'])
model.fit(train_generator, validation_data = test_generator, epochs =5,callbacks=[tensorboard_callback,reduceplat_callback])
scores = model.evaluate(test_generator, verbose=0)
print("final loss",scores[0],"final accuracy",scores[1])

# Convert the model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()
# Save the model.
with open('gesture_detector.tflite', 'wb') as f:
  f.write(tflite_model)

vgg16=tf.keras.applications.VGG16(include_top=False,weights="imagenet",input_shape=input_shape)
# the base model
vgg16.trainable=False
headModel1 = vgg16.output
#headModel = AveragePooling2D(pool_size=(7, 7))(headModel)
headModel1= Flatten(name="flatten")(headModel1)
headModel1 = Dense(512, activation="relu")(headModel1)
headModel1 = Dropout(0.3)(headModel1)
headModel1 = Dense(128, activation="relu")(headModel1)
headModel1 = Dropout(0.2)(headModel1)
headModel1 = Dense(28, activation="softmax")(headModel1)
# place the head FC model on top of the base model (this will become
# the actual model we will train)
model1 = Model(inputs=vgg16.input, outputs=headModel1)

model1.compile(optimizer =Adamax(lr=0.0001), loss = 'categorical_crossentropy', metrics = ['acc'])
model1.fit(train_generator, validation_data = test_generator, epochs =5)
scores = model1.evaluate(test_generator, verbose=0)
print("final loss",scores[0],"final accuracy",scores[1])

y_pred=model.predict(test_generator,len(test_generator.classes) // batch_size)
y_pred=np.argmax(y_pred,axis=1)
print(y_pred)
labels=(test_generator.classes)
print(labels)



!pip install pafy
!pip install --upgrade youtube_dl
!pip install python-vlc
WhatsApp Video 2021-05-02 at 12.19.55 AM

# importing libraries
import cv2
import numpy as np
   
# Create a VideoCapture object and read from input file
cap = cv2.VideoCapture('v2.mp4')
s='' 
# Check if camera opened successfully
if (cap.isOpened()== False): 
  print("Error opening video  file")
   
# Read until video is completed
while(cap.isOpened()):
      
  # Capture frame-by-frame
  ret, frame = cap.read()
  if ret == True:
   
    # Display the resulting frame
    plt.imshow(frame)
    plt.show()
    img = cv2.resize(frame,(224,224))
    img = np.reshape(img,[1,224,224,3])
    feature_map= model.predict(img)
    y_pred=np.argmax(feature_map,axis=1)
    print(y_pred)
    s=s+y_pred[0]
    # Press Q on keyboard to  exit
    if cv2.waitKey(25) & 0xFF == ord('q'):
      break
   
  # Break the loop
  else: 
    break
   
# When everything done, release 
# the video capture object
cap.release()
   
# Closes all the frames
cv2.destroyAllWindows()
print(s)

from gtts import gTTS
  
# This module is imported so that we can 
# play the converted audio
import os
  
# The text that you want to convert to audio
mytext = 'Welcome to geeksforgeeks!'
  
# Language in which you want to convert
language = 'en'
  
# Passing the text and language to the engine, 
# here we have marked slow=False. Which tells 
# the module that the converted audio should 
# have a high speed
myobj = gTTS(text=mytext, lang=language, slow=False)
  
# Saving the converted audio in a mp3 file named
# welcome 
myobj.save("welcome.mp3")

model = Sequential()

model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(64,64,3)))
model.add(MaxPool2D(pool_size=(2, 2), strides=2))

model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', padding = 'same'))
model.add(MaxPool2D(pool_size=(2, 2), strides=2))

model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', padding = 'valid'))
model.add(MaxPool2D(pool_size=(2, 2), strides=2))

model.add(Flatten())

model.add(Dense(64,activation ="relu"))
model.add(Dense(128,activation ="relu"))
model.add(Dropout(0.2))
model.add(Dense(128,activation ="relu"))
model.add(Dropout(0.3))
model.add(Dense(29,activation ="softmax"))

# ADAM - 0.80 - 25 epochs, 0.85 - 20 epochs
# RMSprop - 0.81- 25 epochs, 0.81- 20 epochs
# SGD - 0.83- 25 epochs, 0.83- 20 epochs

model.compile(optimizer=SGD(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
#reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, min_lr=0.0001)
#early_stop = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=0, mode='auto')
model.fit(train_generator, epochs=25, validation_data = test_generator, batch_size=batch_size,steps_per_epoch=steps,verbose=1)

print("A")

